# -*- coding: utf-8 -*-
"""model_training.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mXcshbMZXffyo9fesEoIAP2JntGb3XAA
"""

import tensorflow as tf
import pandas as pd
from transformers import BertTokenizer, BertForTokenClassification
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import pandas as pd
from transformers import BertTokenizer, TFBertModel
from tqdm import tqdm
tf.get_logger().setLevel('ERROR')

bert_model = TFBertModel.from_pretrained('bert-base-uncased')
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

max_length = 256

"""# Load & Preproces data"""

def text_preprocessing(text:str):
  tokenized_text = tokenizer.tokenize(text)
  input_ids = tokenizer.convert_tokens_to_ids(tokenized_text)
  return input_ids

"""Convert Labels"""

def preprocesing(df, max_length = None):
  input_ids = [text_preprocessing(text) for text in tqdm(df['text'], desc="Processing texts")]
  max_inner_length = max_length
  if max_length == None:
    max_inner_length = max(len(ids) for ids in input_ids)
  attention_masks = []
  for ids in input_ids:
      attention_mask = [1] * len(ids) + [0] * (max_inner_length - len(ids))
      attention_masks.append(attention_mask)
  attention_masks_tensor = tf.constant(attention_masks, dtype=tf.int32)
  input_ids = tf.keras.preprocessing.sequence.pad_sequences(input_ids, maxlen = max_inner_length, padding='post', truncating='post')
  labels_tensor = tf.cast(df["label"], tf.int32)
  return tf.convert_to_tensor(input_ids, dtype=tf.int32), tf.convert_to_tensor(attention_masks_tensor, dtype=tf.int32), tf.convert_to_tensor(labels_tensor, dtype=tf.int32), max_inner_length

"""Train - test split"""

train = pd.read_csv("train.csv")
test = pd.read_csv("test.csv")

train_input_ids, train_attention_masks, train_labels, max_inner_length = preprocesing(train)
test_input_ids, test_attention_masks, test_labels, max_inner_length = preprocesing(test, max_inner_length)

"""# Model Definition"""

for layer in bert_model.layers:
    layer.trainable = False

input_layer = tf.keras.layers.Input(shape=(max_inner_length,), dtype=tf.int32)
attention_mask = tf.keras.layers.Input(shape=(max_inner_length,), dtype=tf.int32)
bert_output = bert_model(input_layer, attention_mask=attention_mask)[0]
x = tf.keras.layers.Dense(124, activation="relu")(bert_output[:, 0, :])
x = tf.keras.layers.Dense(124, activation="relu")(x)
output_layer = tf.keras.layers.Dense(1, activation='sigmoid')(x)
model = tf.keras.models.Model(inputs=[input_layer, attention_mask], outputs=output_layer)

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss="binary_crossentropy", metrics=['accuracy'])

"""# Train"""

model.fit([train_input_ids, train_attention_masks], train_labels, epochs=5,  validation_data=[[test_input_ids, test_attention_masks], test_labels])